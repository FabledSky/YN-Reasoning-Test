# ComprehensiveAgents.md

**Purpose:**
This document gives **clear, precise instructions for an AI coding agent** on how to implement and maintain the files in this repository. It defines:

- File formats (`data/*.json`)
- Documentation expectations (`docs/*.md`)
- Code modules (`src/generator/*.py`, `src/scoring/*.py`, `src/ui/api_example.md`)
- Tests (`tests/test_item_validity.py`)

All code and data must support the core goals:

1. Generate large banks of **True/False (Yes/No) reasoning items**.
2. Ensure items are **ESL-friendly**, **single-sentence**, and **culture-reduced**.
3. Maintain strong structure for **future psychometric analysis**.

---

## Global Conventions for the Agent

Before you touch individual files, follow these global rules:

1. **Language of items**
   - Items are **one sentence**.
   - Items must be understandable for **ESL** users (simple English).
   - No idioms, slang, or complex grammar.
   - Use **digits** for numbers (e.g. `7`, not `"seven"`).

2. **True/False item structure**
   - Every item has a **statement** and a **correct answer**.
   - Correct answer is **boolean** (`true` / `false`) in JSON and `bool` in Python.

3. **Item families**
   - Supported families (for now):
     - `arithmetic`
     - `number_properties`
     - `patterns`
     - `transitivity`
     - `sets_logic`
     - `ordering`
     - `points_scoring`
     - `basic_logic`
   - These families must be reflected consistently in:
     - JSON data (`type` / `family` fields)
     - Generator modules (`arithmetic.py`, `patterns.py`, `logic_sets.py`, etc.)
     - Metadata and tests.

4. **Python style**
   - Use **Python 3.10+** features where reasonable (type hints, dataclasses).
   - Follow **PEP 8** style.
   - Write **small, pure functions** for item generation.
   - Prefer **dataclasses** or small typed models for item objects.

---

## `data/` Directory

### Overall Goal

This directory stores:

- Raw item bank (draft items)
- Cleaned, approved item bank
- Metadata about items

All JSON files must use **UTF-8** encoding and **two-space indentation**.

---

### `data/items_raw.json`

**Purpose:**
Store **draft items** generated by humans or AI before full review.

**Format:**
An array of item objects.

**Required fields for each item:**

```json
{
  "id": "string-unique-id",
  "text": "Single sentence item text.",
  "answer": true,
  "family": "arithmetic",
  "difficulty": null,
  "status": "draft",
  "source": "auto_generated",
  "notes": ""
}

```

**Field definitions:**

-   `id` (string)

    -   Unique identifier (UUID4 recommended, e.g. `"e2a8f7d2-8d39-4a9e-b7d1-f9e5f6a9b234"`).

-   `text` (string)

    -   A **single sentence** True/False statement.

    -   Must follow ESL and style guidelines (see `docs/language_guidelines.md`).

-   `answer` (boolean)

    -   `true` if the statement is factually correct, `false` otherwise.

-   `family` (string)

    -   One of: `"arithmetic"`, `"number_properties"`, `"patterns"`, `"transitivity"`, `"sets_logic"`, `"ordering"`, `"points_scoring"`, `"basic_logic"`.

-   `difficulty` (number or null)

    -   Initially `null` for drafts; filled later from analysis (e.g. IRT difficulty).

-   `status` (string)

    -   For `items_raw.json` it is almost always `"draft"`.

-   `source` (string)

    -   `"human"` or `"auto_generated"` or `"mixed"`.

-   `notes` (string)

    -   Optional comments from reviewers or generation hints.

**Agent instructions:**

-   When adding new draft items, **append** to this array.

-   **Do not** modify existing `id`s.

-   Ensure every item:

    -   Has valid `family` value.

    -   Has non-empty `text`.

    -   Uses `answer` as a **boolean**, not `"True"`/`"False"` strings.

* * * * *

### `data/items_clean.json`

**Purpose:**\
Store **approved items** ready for deployment/experiments.

**Format:**\
An array of item objects, similar to `items_raw.json`, but with stricter constraints.

**Required fields:**

```
{
  "id": "string-unique-id",
  "text": "Single sentence item text.",
  "answer": true,
  "family": "arithmetic",
  "difficulty": 0.0,
  "status": "approved",
  "tags": ["esl_safe", "single_sentence"],
  "version": 1
}

```

**Field definitions:**

-   `id` (string)

    -   Same identifier as in `items_raw.json`.

    -   Items move from `items_raw` to `items_clean` by copying and updating.

-   `text` (string)

    -   Fully reviewed and **locked** for this version.

-   `answer` (boolean)

    -   Final correct answer.

-   `family` (string)

    -   Same allowed values as above.

-   `difficulty` (number)

    -   Optional numeric difficulty (e.g. 0.0 as placeholder, or IRT `b` parameter).

-   `status` (string)

    -   `"approved"` or `"retired"`.

-   `tags` (array of strings)

    -   E.g. `["esl_safe", "single_sentence", "short_numbers"]`.

-   `version` (integer)

    -   Increments when a text is meaningfully changed.

**Agent instructions:**

-   Only add items to `items_clean.json` after they pass **all tests** and **manual style review**.

-   If you change `text` meaningfully:

    -   Increment `version`.

    -   Note change in `notes` or in a separate change log (if added later).

-   Keep `status = "approved"` for active items.

-   Set `status = "retired"` for items no longer in use, but do not delete them for historical/analysis reasons.

* * * * *

### `data/item_metadata.json`

**Purpose:**\
Central metadata keyed by `id`, decoupled from text.

**Format:**

```
{
  "e2a8f7d2-8d39-4a9e-b7d1-f9e5f6a9b234": {
    "family": "arithmetic",
    "difficulty": 0.15,
    "discrimination": 1.2,
    "status": "approved",
    "usage_count": 1200,
    "last_used": "2025-11-15",
    "flags": ["high_discrimination"]
  }
}

```

**Field definitions per item `id`:**

-   `family` (string)

    -   Mirrors item family.

-   `difficulty` (number)

    -   Difficulty estimate (e.g. IRT).

-   `discrimination` (number)

    -   Discrimination parameter (if available).

-   `status` (string)

    -   `"draft"`, `"approved"`, `"retired"`, etc.

-   `usage_count` (integer)

    -   How many times this item has been administered.

-   `last_used` (string, date yyyy-mm-dd)

-   `flags` (array of strings)

    -   E.g. `["potential_bias", "esl_check_needed"]`.

**Agent instructions:**

-   Use this file as a **lookup table** for analytics and selection.

-   Always ensure that each `id` here also exists in either `items_raw.json` or `items_clean.json`.

-   When adding new items, you may also add a basic entry here with:

    -   `difficulty: null`, `discrimination: null`, `usage_count: 0`, `flags: []`.

* * * * *

`docs/` Directory
-----------------

### `docs/language_guidelines.md`

**Purpose:**\
Detailed ESL style guide for item text.

**Agent instructions:**

Create/maintain this file with sections:

1.  **Goal**

    -   Explain that the test must be understandable for ESL users with basic English.

2.  **Allowed Grammar Structures**

    -   Simple present tense.

    -   Basic conditional: `If X, then Y.`

    -   Simple comparisons: `A is taller than B.`

3.  **Allowed Vocabulary**

    -   List allowed words: numbers, math words (`plus`, `minus`, `times`, `half`, `double`), logic words (`if`, `then`, `and`, `or`, `all`, `some`, `no`), and basic nouns.

    -   Clarify that items must **prefer** these words and avoid more complex synonyms.

4.  **Forbidden or Discouraged Elements**

    -   Idioms.

    -   Complex conjunctions (`however`, `nevertheless`, etc.).

    -   Very long sentences (e.g. > 25 words).

    -   More than one "if...then" clause per sentence.

5.  **Positive Examples**

    -   Several example item texts that follow the rules.

6.  **Negative Examples**

    -   Examples of items that break the rules, with explanations why.

The agent should keep this file **human-readable**, with headings and bullet lists.

* * * * *

### `docs/item_templates.md`

**Purpose:**\
Define formal **templates** for item generation.

**Agent instructions:**

Structure this file by item family:

-   One section per family: `Arithmetic`, `Number Properties`, `Patterns`, `Transitivity`, `Sets Logic`, `Ordering`, `Points Scoring`, `Basic Logic`.

-   For each section include:

    1.  **Description** of the family.

    2.  **Template syntax** in pseudo-code, e.g.:

        ```
        TEMPLATE: "X plus Y is greater than A plus B."
        VARIABLES:
          X, Y, A, B are integers
        CONSTRAINTS:
          Ensure expression (X + Y > A + B) is true when generating a TRUE item.

        ```

    3.  **Examples of TRUE items**.

    4.  **Examples of FALSE items** (and how to create them by changing variables).

This document is used by the agent to design generator functions.

* * * * *

### `docs/psychometrics_plan.md`

**Purpose:**\
Outline psychometric analysis strategy.

**Agent instructions:**

Create sections covering:

1.  **Overview**

2.  **Data Collection Plan**

    -   How responses will be collected (e.g., via API or UI).

    -   Fields: user ID (or anonymized), item ID, response, response time.

3.  **Item Response Theory (IRT) Plan**

    -   Estimation of difficulty and discrimination.

4.  **Reliability and Validity Checks**

    -   Cronbach's alpha or similar metrics.

5.  **Bias and DIF Analysis**

    -   How to test for item bias across groups.

6.  **Updating Metadata**

    -   How results feed back into `item_metadata.json`.

This file is text-only; the agent should not include code here, only conceptual plans.

* * * * *

`src/` Directory
----------------

### Common item model (recommended)

Before describing individual modules, the agent should use a consistent item model, e.g.:

```
from dataclasses import dataclass
from typing import Literal

Family = Literal[
    "arithmetic",
    "number_properties",
    "patterns",
    "transitivity",
    "sets_logic",
    "ordering",
    "points_scoring",
    "basic_logic",
]

@dataclass
class Item:
    id: str
    text: str
    answer: bool
    family: Family
    difficulty: float | None = None
    status: str = "draft"

```

This can live in a shared module (e.g. `src/generator/base.py`) if needed.

* * * * *

### `src/generator/` Modules

Each generator module must:

-   Provide functions that **generate valid items** for its family.

-   Respect **language guidelines** and **templates**.

-   Return items as Python `dict` or `Item` instances consistent with the schema.

#### `src/generator/arithmetic.py`

**Purpose:**\
Generate arithmetic comparison items (e.g. sums, products, half/double).

**Agent instructions:**

-   Create functions like:

    ```
    from typing import Literal
    import uuid

    def generate_arithmetic_item(
        true_item: bool = True,
        difficulty: Literal["easy", "medium", "hard"] = "easy",
    ) -> dict:
        ...

    ```

-   Responsibilities:

    -   Randomly choose numbers (e.g. between 1 and 50) based on `difficulty`.

    -   Build one of the templates defined in `docs/item_templates.md`, e.g.:

        -   `"X plus Y is greater than A plus B."`

        -   `"Half of N is M."`

        -   `"Double of N is M."`

    -   Compute a **correct true statement**, then:

        -   If `true_item = True`, keep it as is.

        -   If `true_item = False`, modify one number to make the statement false.

    -   Generate a unique `id` (e.g. using `uuid.uuid4()`).

    -   Set `family = "arithmetic"` and `answer = true_item`.

-   Ensure:

    -   Sentence is simple, one line, ESL-friendly.

    -   No division by zero or invalid math.

* * * * *

#### `src/generator/patterns.py`

**Purpose:**\
Generate list-and-pattern items.

**Agent instructions:**

-   Provide a function like:

    ```
    def generate_pattern_item(
        true_item: bool = True,
        difficulty: Literal["easy", "medium", "hard"] = "easy",
    ) -> dict:
        ...

    ```

-   Behavior:

    -   Choose a pattern type:

        -   add constant (e.g. +2, +3)

        -   subtract constant

        -   multiply by constant (e.g. Ã—2)

    -   Generate a short list (e.g. 4--5 numbers).

    -   Build text like:

        -   `"In this list: 1, 3, 5, 7, each number is 2 more than the number before it."`

    -   For `true_item = True`:

        -   Ensure pattern is correct.

    -   For `true_item = False`:

        -   Either break the pattern (change one list element), or write an incorrect pattern description.

-   Set `family = "patterns"`.

* * * * *

#### `src/generator/logic_sets.py`

**Purpose:**\
Generate items about sets and categories using made-up names.

**Agent instructions:**

-   Provide a function like:

    ```
    def generate_sets_logic_item(
        true_item: bool = True,
        difficulty: Literal["easy", "medium", "hard"] = "easy",
    ) -> dict:
        ...

    ```

-   Behavior:

    -   Use names like `"Zors"`, `"Blins"`, `"Kets"`, `"Rims"`, `"Laks"`, etc.

    -   Use templates from `docs/item_templates.md`, e.g.:

        -   `"If all Zors are Blins and all Blins are Kets, then all Zors are Kets."`

        -   `"If all Rims are Laks and some Laks are Fods, then some Fods are Rims for sure."`

    -   For `true_item = True`, enforce logically valid conclusion.

    -   For `true_item = False`, enforce logically invalid conclusion.

-   Set `family = "sets_logic"`.

* * * * *

*(You can add additional generator modules later, e.g. `ordering.py`, `points_scoring.py`, and `basic_logic.py`, following the same pattern.)*

* * * * *

### `src/scoring/scoring_model.py`

**Purpose:**\
Provide functions to compute **scores** from item responses.

**Agent instructions:**

-   Implement at least:

    ```
    from typing import Iterable

    def score_raw(responses: Iterable[dict]) -> int:
        """
        responses: iterable of {"id": str, "answer": bool, "correct": bool}
        Returns: total number of correct responses.
        """
        ...

    def score_proportion(responses: Iterable[dict]) -> float:
        """
        Returns: proportion correct (0.0 to 1.0)
        """
        ...

    ```

-   Future extensions (stubs allowed):

    -   `score_irt(...)` for IRT-based scoring using difficulty/discrimination.

-   The agent must keep this module **side-effect free**:

    -   No file I/O inside scoring functions.

    -   No network calls.

    -   Pure computation only.

* * * * *

### `src/ui/api_example.md`

**Purpose:**\
Explain how an external system (web app or service) can:

-   Request items.

-   Present them to users.

-   Send back responses for scoring.

**Agent instructions:**

-   Include:

    1.  Example of **requesting an item** (pseudo-API):

        ```
        GET /api/items/next?family=arithmetic&difficulty=easy

        ```

    2.  Example JSON response:

        ```
        {
          "id": "e2a8f7d2-8d39-4a9e-b7d1-f9e5f6a9b234",
          "text": "7 plus 5 is greater than 3 plus 8.",
          "answer_type": "boolean"
        }

        ```

    3.  Example of **submitting a response**:

        ```
        POST /api/items/response
        Content-Type: application/json

        {
          "user_id": "user-123",
          "item_id": "e2a8f7d2-8d39-4a9e-b7d1-f9e5f6a9b234",
          "user_answer": true
        }

        ```

    4.  Example scoring call or batch scoring usage.

-   This file is documentation only; the agent should write it as clear markdown.

* * * * *

`tests/` Directory
------------------

### `tests/test_item_validity.py`

**Purpose:**\
Automated tests that ensure all items conform to rules.

**Agent instructions:**

Implement tests using `pytest`. At minimum, include tests for:

1.  **JSON schema validation**

    -   Load `data/items_raw.json` and `data/items_clean.json`.

    -   For each item, assert:

        -   `id` is a non-empty string.

        -   `text` is a non-empty string.

        -   `answer` is `bool`.

        -   `family` is in the allowed set.

        -   `status` is a known value (`"draft"`, `"approved"`, `"retired"`).

2.  **Single-sentence rule**

    -   Check that `text` contains **exactly one sentence**.

        -   Simple heuristic: count sentence-ending punctuation (`.` `?` `!`).

        -   Ensure there is exactly one such mark (or one at the end).

    -   Reject or fail if multiple sentence-ending punctuation marks appear.

3.  **ESL-friendly length**

    -   Check that word count is below a threshold (e.g. `<= 25` words).

    -   Words can be counted by `text.split()`.

4.  **Allowed characters / format**

    -   Ensure only standard ASCII letters, digits, commas, periods, and basic symbols are used.

    -   (You may relax or refine this if needed.)

5.  **Logical consistency for known patterns (optional but recommended)**

    -   For certain families (e.g. `arithmetic`, `patterns`), parse the text of **known template items** and recompute correctness to ensure there is no mismatch between `answer` and the true logic.

    -   This can be done for:

        -   Items that exactly match known templates.

        -   Or as a separate suite of **generator tests** that verify generated items are internally consistent.

**Example skeleton:**

```
import json
from pathlib import Path

ALLOWED_FAMILIES = {
    "arithmetic",
    "number_properties",
    "patterns",
    "transitivity",
    "sets_logic",
    "ordering",
    "points_scoring",
    "basic_logic",
}

ROOT = Path(__file__).resolve().parents[1]

def load_items(filename: str):
    with open(ROOT / "data" / filename, "r", encoding="utf-8") as f:
        return json.load(f)

def test_items_schema():
    for fname in ("items_raw.json", "items_clean.json"):
        items = load_items(fname)
        for item in items:
            assert isinstance(item["id"], str) and item["id"]
            assert isinstance(item["text"], str) and item["text"]
            assert isinstance(item["answer"], bool)
            assert item["family"] in ALLOWED_FAMILIES

def test_single_sentence():
    items = load_items("items_clean.json")
    for item in items:
        text = item["text"]
        # Simple heuristic: count '.', '?', '!'
        count = sum(text.count(p) for p in ".?!")
        assert count <= 1

```

The agent should expand these tests as needed to enforce all guidelines.

* * * * *

Summary for the AI Coding Agent
-------------------------------

When working on this repository:

-   Treat `docs/` as the **source of truth** for language and template rules.

-   Treat `data/` as the **persistent item store**; keep formats stable and validated.

-   Use `src/generator/` for **pure item generation logic** by family.

-   Use `src/scoring/` for **pure scoring logic**.

-   Use `tests/` to **enforce correctness** of items and consistency of format.

Always ensure:

1.  Items are **single-sentence True/False** statements.

2.  Language is **simple, professional, ESL-friendly**.

3.  Content is **culture-reduced** (no local references, no trivia).

4.  JSON formats are stable and validated by tests.

This document should be your reference when writing or updating any of the files listed in the repository structure.
